{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3558481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import chess.engine\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "343305a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state):\n",
    "        self.state = state\n",
    "        self.visits = 0\n",
    "        self.total_score = 0\n",
    "        self.children = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b966ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MCTS agent\n",
    "class MCTSAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "    def get_action(self, state, temperature=1, num_simulations=100):\n",
    "        state_str = self.convert_state_to_fen(state)\n",
    "        print(\"State FEN:\", state_str)  # Print the state FEN for debugging\n",
    "        root_node = Node(state_str)\n",
    "        temp_board = chess.Board()\n",
    "        temp_board.set_fen(state_str)\n",
    "            \n",
    "        # Selection\n",
    "        while len(node.children) != 0:\n",
    "            action, child = max(node.children.items(), key=lambda x: self._uct_value(x[1], node.visit_count))\n",
    "            temp_board.push(chess.Move.from_uci(action))\n",
    "            node = child\n",
    "\n",
    "        # Expansion\n",
    "        if not temp_board.is_game_over():\n",
    "            legal_moves = [move.uci() for move in temp_board.legal_moves]\n",
    "            for move in legal_moves:\n",
    "                new_state = self._get_state_after_action(state, move)\n",
    "                if move not in node.children:\n",
    "                    node.add_child(move, MCTSNode(new_state))\n",
    "                    break\n",
    "        # Simulation\n",
    "        simulation_result = self._simulate(temp_board)\n",
    "\n",
    "        # Backpropagation\n",
    "        while node is not None:\n",
    "            node.visit_count += 1\n",
    "            node.total_reward += simulation_result\n",
    "            node = node.parent\n",
    "\n",
    "        action = max(root_node.children, key=lambda x: root_node.children[x].visit_count)\n",
    "\n",
    "        return action\n",
    "    def _simulate(self, board):\n",
    "        engine = chess.engine.SimpleEngine.popen_uci(\"C:/Users/sofia/NOVA IMS/1st year/Semester 2/Reinforcement Learning/stockfish_15.1_win_x64_avx2/stockfish_15.1_win_x64_avx2/stockfish-windows-2022-x86-64-avx2\")\n",
    "        result = engine.play(board, chess.engine.Limit(time=2))\n",
    "        engine.quit()\n",
    "        return 1 if result.board().result() == \"1-0\" else -1 if result.board().result() == \"0-1\" else 0\n",
    "\n",
    "    def _uct_value(self, child_node, parent_visit_count):\n",
    "        return child_node.total_reward / child_node.visit_count + np.sqrt(2 * np.log(parent_visit_count) / child_node.visit_count)\n",
    "\n",
    "    def _get_state_after_action(self, state, action):\n",
    "        board = chess.Board()\n",
    "        board.set_fen(chess.Board.fen(chess.Board(state)))\n",
    "        board.push(chess.Move.from_uci(action))\n",
    "        return board.fen()\n",
    "    def index_to_piece(self, index):\n",
    "        piece_mapping = {\n",
    "            1: 'p', 2: 'n', 3: 'b', 4: 'r', 5: 'q', 6: 'k',\n",
    "            7: 'P', 8: 'N', 9: 'B', 10: 'R', 11: 'Q', 12: 'K'\n",
    "        }\n",
    "        return piece_mapping.get(index, ' ')\n",
    "    def convert_state_to_fen(self, state):\n",
    "        state_list = state.reshape((8, 8))\n",
    "        fen_str = \"\"\n",
    "        empty_count = 0\n",
    "        for row in state_list:\n",
    "            fen_row = \"\"\n",
    "            for cell in row:\n",
    "                if cell == 0:\n",
    "                    empty_count += 1\n",
    "                else:\n",
    "                    if empty_count > 0:\n",
    "                        fen_row += str(empty_count)\n",
    "                        empty_count = 0\n",
    "                    fen_row += self.index_to_piece(cell)\n",
    "                    fen_row += self.index_to_piece(cell)\n",
    "            if empty_count > 0:\n",
    "                fen_row += str(empty_count)\n",
    "                empty_count = 0\n",
    "            fen_str += fen_row + \"/\"\n",
    "        fen_str = fen_str.rstrip(\"/\")\n",
    "        fen_str += \" w - - 0 1\"  # Add the turn information (defaulting to White's turn)\n",
    "        return fen_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6620737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chess environment\n",
    "class ChessEnvironment:\n",
    "    def __init__(self):\n",
    "        self.board = chess.Board()\n",
    "\n",
    "    def get_state(self):\n",
    "        state = np.zeros(64, dtype=np.uint8)\n",
    "        for i in range(64):\n",
    "            piece = self.board.piece_at(i)\n",
    "            if piece is not None:\n",
    "                state[i] = self._piece_to_index(piece)\n",
    "        return state\n",
    "    def _piece_to_index(self, piece):\n",
    "        piece_map = {\n",
    "            chess.PAWN: 1,\n",
    "            chess.ROOK: 2,\n",
    "            chess.KNIGHT: 3,\n",
    "            chess.BISHOP: 4,\n",
    "            chess.QUEEN: 5,\n",
    "            chess.KING: 6\n",
    "        }\n",
    "        return piece_map[piece.piece_type] * (1 if piece.color == chess.WHITE else -1)\n",
    "\n",
    "    def step(self, action):\n",
    "        move = chess.Move.from_uci(action)\n",
    "        self.board.push(move)\n",
    "        done = self.board.is_game_over()\n",
    "        reward = 1 if done and self.board.result() == \"1-0\" else -1 if done and self.board.result() == \"0-1\" else 0\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = chess.Board()\n",
    "        return self.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4b7bb9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MCTS node\n",
    "class MCTSNode:\n",
    "    def __init__(self, state):\n",
    "        self.state = state\n",
    "        self.visit_count = 0\n",
    "        self.total_reward = 0\n",
    "        self.children = {}\n",
    "\n",
    "    def add_child(self, action, child_node):\n",
    "        self.children[action] = child_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "827651e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent():\n",
    "    env = ChessEnvironment()\n",
    "    state_size = 64\n",
    "    action_size = 4096\n",
    "    agent = MCTSAgent(state_size, action_size)\n",
    "\n",
    "    # Initialize the neural network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam())\n",
    "    episodes = 1000\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        print(\"Episode:\", episode + 1)\n",
    "\n",
    "        # Generate training data\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for _ in range(100):  # Generate 100 training samples per episode\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = agent.get_action(state)\n",
    "                next_state, reward, done = env.step(action)\n",
    "\n",
    "                X_train.append(state)\n",
    "                y = np.zeros(action_size)\n",
    "                y[action] = reward\n",
    "                y_train.append(y)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "        # Reshape input data\n",
    "        X_train = X_train.reshape(-1, state_size)\n",
    "\n",
    "        # Train the neural network on the entire dataset\n",
    "        model.fit(X_train, y_train, epochs=1, verbose=0)\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save('chess_agent_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "69bb8595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State FEN: nnbbrrqqkkrrbbnn/pppppppppppppppp/8/8/8/8/                /                 w - - 0 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected 'w' or 'b' for turn part of fen: 'nnbbrrqqkkrrbbnn/pppppppppppppppp/8/8/8/8/                /                 w - - 0 1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[217], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Start training the agent\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[216], line 18\u001b[0m, in \u001b[0;36mtrain_agent\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 18\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     21\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n",
      "Cell \u001b[1;32mIn[213], line 12\u001b[0m, in \u001b[0;36mMCTSAgent.get_action\u001b[1;34m(self, state, temperature, num_simulations)\u001b[0m\n\u001b[0;32m     10\u001b[0m root_node \u001b[38;5;241m=\u001b[39m Node(state_str)\n\u001b[0;32m     11\u001b[0m temp_board \u001b[38;5;241m=\u001b[39m chess\u001b[38;5;241m.\u001b[39mBoard()\n\u001b[1;32m---> 12\u001b[0m \u001b[43mtemp_board\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_fen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Selection\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(node\u001b[38;5;241m.\u001b[39mchildren) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BusinessCases\\lib\\site-packages\\chess\\__init__.py:2246\u001b[0m, in \u001b[0;36mBoard.set_fen\u001b[1;34m(self, fen)\u001b[0m\n\u001b[0;32m   2244\u001b[0m         turn \u001b[38;5;241m=\u001b[39m BLACK\n\u001b[0;32m   2245\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2246\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for turn part of fen: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfen\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2248\u001b[0m \u001b[38;5;66;03m# Validate castling part.\u001b[39;00m\n\u001b[0;32m   2249\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: expected 'w' or 'b' for turn part of fen: 'nnbbrrqqkkrrbbnn/pppppppppppppppp/8/8/8/8/                /                 w - - 0 1'"
     ]
    }
   ],
   "source": [
    "# Start training the agent\n",
    "train_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af1d00c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
